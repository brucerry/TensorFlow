{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_graphs(title, history, string):\n",
    "  plt.title(title)\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "def plot_results(title, history):\n",
    "  plot_graphs(title, history, \"accuracy\")\n",
    "  plot_graphs(title, history, \"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "zip_dir = tf.keras.utils.get_file('flower_photos.tgz', origin=_URL, extract=True)\n",
    "zip_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the dataset we have downloaded has following directory structure.\n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>flower_photos</b>\n",
    "|__ <b>daisy</b>\n",
    "|__ <b>dandelion</b>\n",
    "|__ <b>roses</b>\n",
    "|__ <b>sunflowers</b>\n",
    "|__ <b>tulips</b>\n",
    "</pre>\n",
    "\n",
    "As you can see there are no folders containing training and validation data. Therefore, we will have to create our own training and validation set. Let's write some code that will do this.\n",
    "\n",
    "\n",
    "The code below creates a `train` and a `val` folder each containing 5 folders (one for each type of flower). It then moves the images from the original folders to these new folders such that `80%` of the images go to the training set and `20%` of the images go into the validation set. In the end our directory will have the following structure:\n",
    "\n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>flower_photos</b>\n",
    "|__ <b>daisy</b>\n",
    "|__ <b>dandelion</b>\n",
    "|__ <b>roses</b>\n",
    "|__ <b>sunflowers</b>\n",
    "|__ <b>tulips</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>daisy</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ <b>dandelion</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ <b>roses</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ <b>sunflowers</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ <b>tulips</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    " |__ <b>validation</b>\n",
    "    |______ <b>daisy</b>: [507.jpg, 508.jpg, 509.jpg ....]\n",
    "    |______ <b>dandelion</b>: [719.jpg, 720.jpg, 721.jpg ....]\n",
    "    |______ <b>roses</b>: [514.jpg, 515.jpg, 516.jpg ....]\n",
    "    |______ <b>sunflowers</b>: [560.jpg, 561.jpg, 562.jpg .....]\n",
    "    |______ <b>tulips</b>: [640.jpg, 641.jpg, 642.jpg ....]\n",
    "</pre>\n",
    "\n",
    "Since we don't delete the original folders, they will still be in our `flower_photos` directory, but they will be empty. The code below also prints the total number of flower images we have for each type of flower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']\n",
    "\n",
    "base_dir = os.path.join(os.path.dirname(zip_dir), 'flower_photos')\n",
    "\n",
    "# total_train = 0\n",
    "# total_val = 0\n",
    "\n",
    "# for cl in classes:\n",
    "#   img_path = os.path.join(base_dir, cl)\n",
    "#   images = glob.glob(img_path + '/*.jpg')\n",
    "#   print(\"{}: {} Images\".format(cl, len(images)))\n",
    "#   train, val = images[:round(len(images)*0.8)], images[round(len(images)*0.8):]\n",
    "\n",
    "#   for t in train:\n",
    "#     if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
    "#       os.makedirs(os.path.join(base_dir, 'train', cl))\n",
    "#     shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
    "#     total_train += 1\n",
    "\n",
    "#   for v in val:\n",
    "#     if not os.path.exists(os.path.join(base_dir, 'validation', cl)):\n",
    "#       os.makedirs(os.path.join(base_dir, 'validation', cl))\n",
    "#     shutil.move(v, os.path.join(base_dir, 'validation', cl))\n",
    "#     total_val += 1\n",
    "\n",
    "# print(\"Total training images:\", total_train)\n",
    "# print(\"Total validation images:\", total_val)\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set ImageDataGenerator with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "IMG_SHAPE  = 150 # Our training data consists of images with width of 150 pixels and height of 150 pixels\n",
    "\n",
    "image_gen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.5,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_data = image_gen_train.flow_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    directory=train_dir,\n",
    "    shuffle=True,\n",
    "    target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "augmented_images = [train_data[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_data = image_gen_val.flow_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    directory=validation_dir,\n",
    "    target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_SHAPE,IMG_SHAPE, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(5))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data,\n",
    "    #callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\"\", history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.evaluate(test_data.batch(1024), verbose=2)\n",
    "\n",
    "# for name, value in zip(model.metrics_names, results):\n",
    "#   print(\"%s: %.3f\" % (name, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
