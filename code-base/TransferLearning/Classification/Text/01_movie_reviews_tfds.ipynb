{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.tensorflow.org/tutorials/keras/text_classification_with_hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import tensorflow_text as text\n",
    "\n",
    "# import tensorflow.compat.v1 as tf1\n",
    "# config = tf1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf1.Session(config=config)\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# print(\"Num GPUs Available: \", len(gpus))\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Returns separate loss curves for training and validation metrics.\n",
    "\n",
    "  Args:\n",
    "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
    "  \"\"\" \n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  accuracy = history.history['accuracy']\n",
    "  val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "  epochs = range(len(history.history['loss']))\n",
    "\n",
    "  # Plot loss\n",
    "  plt.plot(epochs, loss, label='training_loss')\n",
    "  plt.plot(epochs, val_loss, label='val_loss')\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()\n",
    "\n",
    "  # Plot accuracy\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the IMDB dataset\n",
    "\n",
    "It uses the [IMDB](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) dataset that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into `25,000 reviews for training` and `25,000 reviews for testing`. The training and testing sets are balanced, meaning they contain an `equal` number of positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into 60% and 40% to end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "(train_data, validation_data, test_data), info = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "print(info.features)\n",
    "\n",
    "num_classes = info.features[\"label\"].num_classes\n",
    "classes = info.features[\"label\"].names\n",
    "\n",
    "print(num_classes)\n",
    "print(classes)\n",
    "\n",
    "num_train_data = len(train_data)\n",
    "num_validation_data = len(validation_data)\n",
    "num_test_data = len(test_data)\n",
    "\n",
    "print(\"Train data size:\", num_train_data)\n",
    "print(\"Validation data size:\", num_validation_data)\n",
    "print(\"Test data size:\", num_test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_batch = train_data.shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_batch = validation_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_batch = test_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# sen_batch, label_batch = next(iter(train_batch.take(1)))\n",
    "# sen_batch\n",
    "# label_batch\n",
    "\n",
    "# train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "# train_examples_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "For this example you use a pre-trained text embedding model **(Saved Model)** from TensorFlow Hub called [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2).\n",
    "\n",
    "There are many other pre-trained text embeddings from TFHub that can be used in this tutorial:\n",
    "\n",
    "* [google/nnlm-en-dim128/2](https://tfhub.dev/google/nnlm-en-dim128/2) - trained with the same NNLM architecture on the same data as [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2), but with a larger embedding dimension. Larger dimensional embeddings can **improve** on your task but it may **take longer** to train your model.\n",
    "\n",
    "* [google/nnlm-en-dim128-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2) - the same as [google/nnlm-en-dim128/2](https://tfhub.dev/google/nnlm-en-dim128/2), but with additional text normalization such as removing punctuation. This can help if the text in your task contains **additional characters or punctuation**.\n",
    "\n",
    "* [google/universal-sentence-encoder/4](https://tfhub.dev/google/universal-sentence-encoder/4) - a **much** larger model yielding 512 dimensional embeddings trained with a deep averaging network (DAN) encoder.\n",
    "\n",
    "And many more! Find more [text embedding models](https://tfhub.dev/s?module-type=text-embedding) on TFHub.\n",
    "\n",
    "Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "# url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "# url = 'https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2'\n",
    "# url = 'https://tfhub.dev/google/universal-sentence-encoder/4' # model fit error\n",
    "hub_layer = hub.KerasLayer(url, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "# for units in [32, 16]:\n",
    "#     model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and Optimizer\n",
    "\n",
    "Since this is a binary classification problem and the model outputs logits (a single-unit layer with a linear activation), you'll use the `binary_crossentropy` loss function.\n",
    "\n",
    "* This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is **better** for dealing with probabilitiesâ€”it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_batch,\n",
    "    epochs=30,\n",
    "    validation_data=validation_batch,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = model.evaluate(test_batch, verbose=1)\n",
    "\n",
    "for name, value in zip(model.metrics_names, eval):\n",
    "  print(\"%s: %.10f\" % (name, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
