{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "  Args:\n",
    "      y_true: true labels in the form of a 1D array\n",
    "      y_pred: predicted labels in the form of a 1D array\n",
    "\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Returns separate loss curves for training and validation metrics.\n",
    "\n",
    "  Args:\n",
    "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
    "  \"\"\" \n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  accuracy = history.history['accuracy']\n",
    "  val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "  epochs = range(len(history.history['loss']))\n",
    "\n",
    "  # Plot loss\n",
    "  plt.plot(epochs, loss, label='training_loss')\n",
    "  plt.plot(epochs, val_loss, label='val_loss')\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()\n",
    "\n",
    "  # Plot accuracy\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Kaggle NLP-Getting-Started Dataset\n",
    "\n",
    "The text samples of Tweets labelled as disaster or not disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\bruce\\\\.keras\\\\datasets\\\\nlp_getting_started.zip'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip'\n",
    "\n",
    "zip_dir = tf.keras.utils.get_file(origin=url, extract=True)\n",
    "zip_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4342 sentences labelled as 0 in Training dataset\n",
      "There are 3271 sentences labelled as 1 in Training dataset\n",
      "---\n",
      "Number of Training data: 6090\n",
      "Number of Validation data: 1523\n",
      "Number of Testing data: 3263\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.path.dirname(zip_dir)\n",
    "train_csv = os.path.join(base_dir, 'train.csv')\n",
    "test_csv = os.path.join(base_dir, 'test.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "for label, count in enumerate(train_df.target.value_counts()):\n",
    "    print(f'There are {count} sentences labelled as {label} in Training dataset')\n",
    "\n",
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # frac=1 -> 100% of the data\n",
    "\n",
    "# Split 80% Training data and 20% Validation data\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    train_df_shuffled['text'].to_numpy(),\n",
    "    train_df_shuffled['target'].to_numpy(),\n",
    "    train_size=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Number of Training data:', len(train_sentences))\n",
    "print('Number of Validation data:', len(val_sentences))\n",
    "print('Number of Testing data:', len(test_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use 10% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_10_percent = train_df_shuffled[['text', 'target']].sample(frac=0.1, random_state=42)\n",
    "# train_10_percent_sentences = train_10_percent['text']\n",
    "# train_10_percent_labels = train_10_percent['target']\n",
    "\n",
    "# split = int(0.1 * len(train_sentences))\n",
    "# train_10_percent_sentences = train_sentences[:split]\n",
    "# train_10_percent_labels = train_labels[:split]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization/Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common 5 words are: [['', '[UNK]', 'the', 'a', 'in']]\n",
      "The least common 5 words are: [['minded', 'mindblowing', 'milne', 'milledgeville', 'millcityio']]\n"
     ]
    }
   ],
   "source": [
    "# Average number of tokens (words) in the training tweets\n",
    "max_len = round(sum([len(i.split()) for i in train_sentences]) / len(train_sentences))\n",
    "\n",
    "max_vocab_len = 10000\n",
    "\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_len, # bounded the number of most occurrences of words (auto add <OOV>)\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None, # groups of n-words\n",
    "    output_mode='int', # how to map tokens to numbers\n",
    "    output_sequence_length=max_len, # how long the sequences to be\n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "\n",
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "# Get all the unique words in our training data\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "count = 5\n",
    "print(f'The most common {count} words are: {words_in_vocab[:count]}')\n",
    "print(f'The least common {count} words are: {words_in_vocab[-count:]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\bruce\\\\.keras\\\\datasets\\\\08_model_6_USE_feature_extractor.zip'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://storage.googleapis.com/ztm_tf_course/08_model_6_USE_feature_extractor.zip'\n",
    "\n",
    "zip_dir = tf.keras.utils.get_file(origin=url, extract=True)\n",
    "zip_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(os.path.dirname(zip_dir), '08_model_6_USE_feature_extractor')\n",
    "model = tf.keras.models.load_model(model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 2s 25ms/step - loss: 0.4099 - accuracy: 0.8201\n",
      "loss: 0.4099445343\n",
      "accuracy: 0.8200919032\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'): # this line fix error\n",
    "  eval = model.evaluate(val_sentences, val_labels, verbose=1)\n",
    "\n",
    "for name, value in zip(model.metrics_names, eval):\n",
    "  print(\"%s: %.10f\" % (name, value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/CPU:0'): # this line fix error\n",
    "    pred_probs = model.predict(val_sentences)\n",
    "preds = tf.squeeze(tf.round(pred_probs))\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 82.00919238345371,\n",
       " 'precision': 0.8211717138273148,\n",
       " 'recall': 0.8200919238345371,\n",
       " 'f1': 0.8183988036567054}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = calculate_results(val_labels, preds)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most wrong predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame of different params for each of the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.747162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@camilacabello97 Internally and externally scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radiation emergency #preparedness starts with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>@SidelineSavage what like a pipe made of peanu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>Avalanche City - Sunset http://t.co/48h3tLvLXr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>The Whirlwind! Scourge of Europe! RT @whedones...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>ENGLAND EAST COAST. Dogger Bank Westward. 1. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1523 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  pred  \\\n",
       "0     DFR EP016 Monthly Meltdown - On Dnbheaven 2015...       0   0.0   \n",
       "1     FedEx no longer to transport bioterror germs i...       0   1.0   \n",
       "2     Gunmen kill four in El Salvador bus attack: Su...       1   1.0   \n",
       "3     @camilacabello97 Internally and externally scr...       1   0.0   \n",
       "4     Radiation emergency #preparedness starts with ...       1   1.0   \n",
       "...                                                 ...     ...   ...   \n",
       "1518  @SidelineSavage what like a pipe made of peanu...       0   0.0   \n",
       "1519  Avalanche City - Sunset http://t.co/48h3tLvLXr...       1   0.0   \n",
       "1520  The Whirlwind! Scourge of Europe! RT @whedones...       0   0.0   \n",
       "1521  ENGLAND EAST COAST. Dogger Bank Westward. 1. S...       0   0.0   \n",
       "1522  Suicide bomber kills 15 in Saudi security site...       1   1.0   \n",
       "\n",
       "      pred_probs  \n",
       "0       0.159757  \n",
       "1       0.747162  \n",
       "2       0.988749  \n",
       "3       0.196229  \n",
       "4       0.707808  \n",
       "...          ...  \n",
       "1518    0.114509  \n",
       "1519    0.287812  \n",
       "1520    0.249553  \n",
       "1521    0.432494  \n",
       "1522    0.974736  \n",
       "\n",
       "[1523 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'text': val_sentences,\n",
    "    'target': val_labels,\n",
    "    'pred': preds,\n",
    "    'pred_probs': tf.squeeze(pred_probs)\n",
    "})\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the wrong predictions and sort by prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>@adorableappple No reported flooding po in the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>We should all have a fire safety plan. RT @Mat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>Mourning notices for stabbing arson victims st...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>Two Jewish Terrorists Charged In Historic-Chur...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.918810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>? High Skies - Burning Buildings ? http://t.co...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.910196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>I get to smoke my shit in peace</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>'I did another one I did another one. You stil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Why are you deluged with low self-image? Take ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Reddit Will Now QuarantineÛ_ http://t.co/pkUA...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ron &amp;amp; Fez - Dave's High School Crush https...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  pred  \\\n",
       "881   @adorableappple No reported flooding po in the...       0   1.0   \n",
       "1307  We should all have a fire safety plan. RT @Mat...       0   1.0   \n",
       "846   Mourning notices for stabbing arson victims st...       0   1.0   \n",
       "1502  Two Jewish Terrorists Charged In Historic-Chur...       0   1.0   \n",
       "31    ? High Skies - Burning Buildings ? http://t.co...       0   1.0   \n",
       "...                                                 ...     ...   ...   \n",
       "233                     I get to smoke my shit in peace       1   0.0   \n",
       "935   'I did another one I did another one. You stil...       1   0.0   \n",
       "38    Why are you deluged with low self-image? Take ...       1   0.0   \n",
       "244   Reddit Will Now QuarantineÛ_ http://t.co/pkUA...       1   0.0   \n",
       "23    Ron &amp; Fez - Dave's High School Crush https...       1   0.0   \n",
       "\n",
       "      pred_probs  \n",
       "881     0.970023  \n",
       "1307    0.939019  \n",
       "846     0.938105  \n",
       "1502    0.918810  \n",
       "31      0.910196  \n",
       "...          ...  \n",
       "233     0.042087  \n",
       "935     0.041997  \n",
       "38      0.038998  \n",
       "244     0.038949  \n",
       "23      0.037186  \n",
       "\n",
       "[274 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_wrong = df[df['target'] != df['pred']].sort_values('pred_probs', ascending=False)\n",
    "most_wrong"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive predictions\n",
    "\n",
    "True label is `0` but prediction is `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0, Pred: 1.0, Prob: 0.9700231552124023\n",
      "Text:\n",
      "@adorableappple No reported flooding po in the area. Ten-4. #mmda\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1.0, Prob: 0.9390192627906799\n",
      "Text:\n",
      "We should all have a fire safety plan. RT @Matt_Kroschel: MOCK WILDFIRE near #Vail as agencies prepare for the worst. http://t.co/SWwyLRk0fv\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1.0, Prob: 0.9381048679351807\n",
      "Text:\n",
      "Mourning notices for stabbing arson victims stir Û÷politics of griefÛª in Israel: Posters for Shira Banki and A... http://t.co/3GZ5zQQTHe\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in most_wrong[:3].itertuples():\n",
    "    _, text, target, pred, pred_probs = row\n",
    "    print(f'Target: {target}, Pred: {pred}, Prob: {pred_probs}')\n",
    "    print(f'Text:\\n{text}\\n')\n",
    "    print('----\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negative predictions\n",
    "\n",
    "True label is `1` but prediction is `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1, Pred: 0.0, Prob: 0.038997940719127655\n",
      "Text:\n",
      "Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0.0, Prob: 0.03894944489002228\n",
      "Text:\n",
      "Reddit Will Now QuarantineÛ_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0.0, Prob: 0.03718578442931175\n",
      "Text:\n",
      "Ron &amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in most_wrong[-3:].itertuples():\n",
    "    _, text, target, pred, pred_probs = row\n",
    "    print(f'Target: {target}, Pred: {pred}, Prob: {pred_probs}')\n",
    "    print(f'Text:\\n{text}\\n')\n",
    "    print('----\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on the Test dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick predictions randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "Pred: 0.0 Prob: 0.9805117249488831\n",
      "Text:\n",
      "wo Pic of 16yr old PKK suicide bomber who detonated bomb in Turkey Army trench released http://t.co/Sj57BoKsiB /'/'//\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Pred: 0.0 Prob: 0.09325683861970901\n",
      "Text:\n",
      "@freeagent1717 @ChaseIngram @amandagiroux28 lol I saw something like that last cops was quarantined taking him out the house etc\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Pred: 0.0 Prob: 0.16871552169322968\n",
      "Text:\n",
      "Sometimes blood ain't no thicker than water and sometimes family will bring you down quicker than strangers ???????\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = test_df['text'].to_list()\n",
    "test_random_samples = random.sample(test_sentences, 3)\n",
    "for text in test_random_samples:\n",
    "    with tf.device('/CPU:0'): # this line fix error\n",
    "        pred_prob = tf.squeeze(model.predict([text]))\n",
    "    pred = tf.round(pred_probs)\n",
    "    print(f'Pred: {pred} Prob: {pred_prob}')\n",
    "    print(f'Text:\\n{text}\\n')\n",
    "    print('----\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
