{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips to tweak\n",
    "\n",
    "- Data and preprocessing-based approaches\n",
    "  - More data\n",
    "  - Adjusting vocabulary size (make sure to consider the overall size of the corpus!)\n",
    "  - Adjusting sequence length (more or less padding or truncation)\n",
    "  - Whether to pad or truncate `pre` or `post` (usually less of an effect than the others)\n",
    "- Model-based approaches\n",
    "  - Adjust the number of embedding dimensions\n",
    "  - Changing use of `Flatten` vs. `GlobalAveragePooling1D` (often better)\n",
    "  - Considering other layers like Dropout\n",
    "  - Adjusting the number of nodes in intermediate fully-connected layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(title, history, string):\n",
    "  plt.title(title)\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "def predict_review(model, reviews, tokenizer, maxlen=100, show_padded_sequence=False, trunc_type='post', padding_type='post'):\n",
    "  # Create the sequences\n",
    "  sample_sequences = tokenizer.texts_to_sequences(reviews)\n",
    "  reviews_padded = pad_sequences(sample_sequences, maxlen=maxlen, padding=padding_type, truncating=trunc_type) \n",
    "  classes = model.predict(reviews_padded)\n",
    "  for x in range(len(reviews_padded)):\n",
    "    # We can see the padded sequence if desired\n",
    "    # Print the sequence\n",
    "    if (show_padded_sequence):\n",
    "      print(reviews_padded[x])\n",
    "    print(reviews[x], classes[x])\n",
    "\n",
    "def train_model(model, training_sequences, testing_sequences, training_labels, testing_labels, epochs=30, learning_rate=0.001):\n",
    "  model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  history = model.fit(training_sequences, training_labels, epochs=epochs, validation_data=(testing_sequences, testing_labels))\n",
    "  return history\n",
    "\n",
    "def plot_results(title, history):\n",
    "  plot_graphs(title, history, \"accuracy\")\n",
    "  plot_graphs(title, history, \"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTMs, CNNs, GRUs with a larger dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this colab, you use different kinds of layers to see how they affect the model.\n",
    "\n",
    "You will use the `glue/sst2` dataset, which is available through `tensorflow_datasets`. \n",
    "\n",
    "The `General Language Understanding Evaluation (GLUE)` benchmark (https://gluebenchmark.com/) is a collection of resources for training, evaluating, and analyzing natural language understanding systems.\n",
    "\n",
    "These resources include the `Stanford Sentiment Treebank (SST)` dataset that consists of sentences from movie reviews and human annotations of their sentiment. This colab uses version 2 of the SST dataset.\n",
    "\n",
    "The splits are:\n",
    "\n",
    "*   train\t67,349\n",
    "*   validation\t872\n",
    "\n",
    "\n",
    "and the column headings are:\n",
    "\n",
    "*   sentence\n",
    "*   label\n",
    "\n",
    "\n",
    "For more information about the dataset, see [https://www.tensorflow.org/datasets/catalog/glue#gluesst2](https://www.tensorflow.org/datasets/catalog/glue#gluesst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset.\n",
    "# It has 70000 items, so might take a while to download\n",
    "dataset, info = tfds.load('glue/sst2', with_info=True)\n",
    "print(info.features)\n",
    "print(info.features[\"label\"].num_classes)\n",
    "print(info.features[\"label\"].names)\n",
    "\n",
    "# Get the training and validation datasets\n",
    "dataset_train, dataset_validation = dataset['train'], dataset['validation']\n",
    "dataset_train\n",
    "\n",
    "# Print some of the entries\n",
    "for example in dataset_train.take(2):  \n",
    "  review, label = example[\"sentence\"], example[\"label\"]\n",
    "  print(\"Review:\", review)\n",
    "  print(\"Label: %d \\n\" % label.numpy())\n",
    "\n",
    "# Get the sentences and the labels\n",
    "# for both the training and the validation sets\n",
    "training_reviews = []\n",
    "training_labels = []\n",
    " \n",
    "validation_reviews = []\n",
    "validation_labels = []\n",
    "\n",
    "# The dataset has 67,000 training entries, but that's a lot to process here!\n",
    "\n",
    "# If you want to take the entire dataset: WARNING: takes longer!!\n",
    "# for item in dataset_train.take(-1):\n",
    "\n",
    "# Take 10,000 reviews\n",
    "for item in dataset_train.take(10000):\n",
    "  review, label = item[\"sentence\"], item[\"label\"]\n",
    "  training_reviews.append(str(review.numpy()))\n",
    "  training_labels.append(label.numpy())\n",
    "\n",
    "print (\"\\nNumber of training reviews is: \", len(training_reviews))\n",
    "\n",
    "# print some of the reviews and labels\n",
    "for i in range(0, 2):\n",
    "  print (training_reviews[i])\n",
    "  print (training_labels[i])\n",
    "\n",
    "# Get the validation data\n",
    "# there's only about 800 items, so take them all\n",
    "for item in dataset_validation.take(-1):  \n",
    "  review, label = item[\"sentence\"], item[\"label\"]\n",
    "  validation_reviews.append(str(review.numpy()))\n",
    "  validation_labels.append(label.numpy())\n",
    "\n",
    "print (\"\\nNumber of validation reviews is: \", len(validation_reviews))\n",
    "\n",
    "# Print some of the validation reviews and labels\n",
    "for i in range(0, 2):\n",
    "  print (validation_reviews[i])\n",
    "  print (validation_labels[i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paras\n",
    "vocab_size = 4000\n",
    "embedding_dim = 16\n",
    "max_length = 50\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_reviews)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "print(word_index)\n",
    "\n",
    "# training set\n",
    "training_sequences = tokenizer.texts_to_sequences(training_reviews)\n",
    "training_sequences = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# validation set\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_reviews)\n",
    "validation_sequences = pad_sequences(validation_sequences, maxlen=max_length)\n",
    "\n",
    "# Make labels into numpy arrays for use with the network later\n",
    "training_labels = np.array(training_labels)\n",
    "validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Embeddings\n",
    "model_simple = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Bidirectional LSTM\n",
    "model_bidi_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)), \n",
    "    tf.keras.layers.Dense(6, activation='relu'), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Multiple Bidirectional LSTM\n",
    "model_multiple_bidi_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, return_sequences=True)), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# CNN\n",
    "model_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(16, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# GRU\n",
    "model_gru = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict some reviews   \n",
    "new_reviews = [ \"\"\"I loved this movie\"\"\",\n",
    "                \"\"\"that was the worst movie I've ever seen\"\"\",\n",
    "                \"\"\"too much violence even for a Bond film\"\"\",\n",
    "                \"\"\"a captivating recounting of a cherished myth\"\"\",\n",
    "                \"\"\"I loved this movie\"\"\",\n",
    "                \"\"\"that was the worst movie I've ever seen\"\"\",\n",
    "                \"\"\"too much violence even for a Bond film\"\"\",\n",
    "                \"\"\"a captivating recounting of a cherished myth\"\"\",\n",
    "                \"\"\"I saw this movie yesterday and I was feeling low to start with, but it was such a wonderful movie that it lifted my spirits and brightened my day, you can\\'t go wrong with a movie with Whoopi Goldberg in it.\"\"\",\n",
    "                \"\"\"I don\\'t understand why it received an oscar recommendation\n",
    "                for best movie, it was long and boring\"\"\",\n",
    "                \"\"\"the scenery was magnificent, the CGI of the dogs was so realistic I\n",
    "                thought they were played by real dogs even though they talked!\"\"\",\n",
    "                \"\"\"The ending was so sad and yet so uplifting at the same time. \n",
    "                I'm looking for an excuse to see it again\"\"\",\n",
    "                \"\"\"I had expected so much more from a movie made by the director \n",
    "                who made my most favorite movie ever, I was very disappointed in the tedious \n",
    "                story\"\"\",\n",
    "                \"I wish I could watch this movie every day for the rest of my life\",\n",
    "              ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(model_simple, training_sequences, validation_sequences, training_labels, validation_labels)\n",
    "plot_results(\"Simple Embeddings\", history)\n",
    "predict_review(model_simple, new_reviews, tokenizer, maxlen=max_length)\n",
    "\n",
    "history = train_model(model_bidi_lstm, training_sequences, validation_sequences, training_labels, validation_labels, learning_rate=0.0003)\n",
    "plot_results(\"Bi-LSTM\", history)\n",
    "predict_review(model_bidi_lstm, new_reviews, tokenizer, maxlen=max_length)\n",
    "\n",
    "history = train_model(model_multiple_bidi_lstm, training_sequences, validation_sequences, training_labels, validation_labels, learning_rate=0.003)\n",
    "plot_results(\"multi-Bi-LSTM\", history)\n",
    "predict_review(model_multiple_bidi_lstm, new_reviews, tokenizer, maxlen=max_length)\n",
    "\n",
    "history = train_model(model_cnn, training_sequences, validation_sequences, training_labels, validation_labels, learning_rate=0.0001)\n",
    "plot_results(\"CNN\", history)\n",
    "predict_review(model_cnn, new_reviews, tokenizer, maxlen=max_length)\n",
    "\n",
    "history = train_model(model_gru, training_sequences, validation_sequences, training_labels, validation_labels, learning_rate=0.00003)\n",
    "plot_results(\"GRU\", history)\n",
    "predict_review(model_gru, new_reviews, tokenizer, maxlen=max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
